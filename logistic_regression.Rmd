---
title: "Intro to logistic regression, MiCM Spring 2023 Workshop"
author: "Alex Diaz-Papkovich"
date: "March 17, 2023"
output:
  html_document:
    #github_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The log function

As a basic review of the $\log$ function:

* It is the inverse of exponentiation
* Unless otherwise stated, assume $\log$ refers to that natural logarithm with base $e$, sometimes written $\ln$
* $\log(1) = 0$
* $\log(0)$ is undefined
* $\log(x*y) = \log(x) + \log(y)$
* $\log(x/y) = \log(x) - \log(y)$
* $\exp(\log(x)) = x$

## The binomial distribution

The binomial distribution models a situation where we have multiple independent Bernoulli trials---trials with binary success/failure outcomes. We have three parameters:
* $y$, the observed number of successes
* $n$, the total number of trials
* $p$, the probability of success

The probability mass function is:

$$
P(Y=y) = {n \choose y}p^y(1-p)^{n-y}
$$

For example, if a drug has $90\%$ of clearing an infection and we have $100$ patients, what is the probability that exactly $90$ patients are cleared of their infection? This is expressed mathematically as:

$$
P(Y=90) = {100 \choose 90}(0.9)^{90}(0.1)^{10}
$$

In R, we can calculate the value directly with `choose(100,90)*(0.9)^(90)*(0.1)^(10)` or (more usefully) with `dbinom(90, 100, 0.9)`. The value should be `0.1318653`.

We can also calculate the probability that $90$ or more patients are cleared by summing up these values. We can do this either by summing the probability that $90$, ..., $100$ are cleared, or using the `pbinom` function.

```{r Binomial distribution}
# Calculate the probability of exactly 90 patients clearing an infection
dbinom(90, 100, 0.9)

# Calculate the probability of 90 or more (i.e. 90, 91, ..., 100) patients clearing an infection
sum(dbinom(x = 90:100, size = 100, prob = 0.9))

# Equivalently, calculate the probability that 90 or more (i.e. more than 89) clear an infection
pbinom(89, 100, 0.9, lower.tail = F)
```

We will not go deep into the binomial distribution, though it is very useful. What we are interested in is estimating the value $p$, which we don't know, in the context of binary or count data. We typically have either a count of all events summarized in a table (e.g. in the form of a [contingency table](https://en.wikipedia.org/wiki/Contingency_table)) or as a list of binary events (e.g. survival data with associated variables like drug treatment, sex, age, etc).

## Logistic regression

Assume that we have binary outcome data ($y$) and explanatory variables ($\mathbf{x}$) for each observation. Each observation is a Bernoulli trial where $y=1$ with a probability $\pi$ and $y=0$ with a probability $(1-\pi)$. Each observation is a **Bernoulli trial**. We believe that the probability of success ($y=1$) is related to $x$, so we write $\pi$ as $\pi(x)$. Since we have many Bernoulli trials, which we assume are independent, our data follow the Binomial distribution.

Like with linear regression, we have $n$ observations and $p$ covariates. An individual observation is called a **case**, and when all explanatory covariates match among cases, we call it a **constellation** (personal note: I haven't heard this term used often---I've heard "class", "group", "crosstab", etc). Some examples:

* Survival data ($y=1$ if a patient lived, $y=0$ if they died) by sex ($x_1=1$ for male, $x_1=0$ for female) and treatment ($x_2=1$ for treatment, $x_2=0$ for placebo)
* Vaccine data ($y=1$ if a patient becomes infected) by vaccination status ($x_1=1$ if they were vaccinated)

We write the explanatory variables as $\mathbf{x}=(x_{1},x_{2},\dots,x_{p})'$, and our function of the probability of success as $\pi=\pi(\mathbf{x}_i)$.


The specifics of the notation (i.e. subscripts) will change depending on how the problem is formulated, but generally our binomial probability looks like this:

$$
P(Y=y) = {n \choose y}[\pi(\mathbf{x})]^{y}[1-\pi(\mathbf{x})]^{n-y}
$$
We are interested in estimating $\pi(\mathbf{x})$; we wish to link the probability $\pi$ to the covariates $\mathbf{x} = x_1, \dots, x_p$. We do this by selecting a **link function**. We would like a function in which:

* $\pi$ depends on $\mathbf{x}$ in a linear manner, because linear functions have nice properties
* $\pi$ falls within the values $[0,1]$, since it is a probability.

There are many link functions. We will use the **logistic function** or the **logit**. It takes the form:

$$
\pi(\mathbf{x}) = \frac{1}{1 + e^{-\mathbf{x}'\mathbf{\beta}}}  
$$ 

It looks like this:

```{r The logistic curve}
curve(1/(1+exp(-x)), from=-5, to=5,  xlab="Explanatory variable x", ylab=expression(pi(x)~"=P(Y=1)"))
```

There are other link functions, such as the probit, complementary log-log (cloglog), the log-log, etc. The logit has some nice properties:

* It is symmetric
* It has simpler theoretical properties
* It is easier to interpret as the log of odds ratios
* It can be used in estimates either prospectively or retrospectively

## Interpreting logistic regression

Through some algebra, we can re-arrange the logit equation to get:

$$
\ln\frac{\pi(\mathbf{x})}{1-\pi(\mathbf{x})} = \mathbf{x}'\mathbf{\beta} = \beta_0 + \beta_1x_1 + ... + \beta_px_p
$$
Here $\frac{\pi(\mathbf{x})}{1- \pi(\mathbf{x})}$ is our **log-odds**. We can also exponentiate both sides to get our **odds**:

$$
\frac{\pi(\mathbf{x})}{1-\pi(\mathbf{x})} = \exp(\mathbf{x}'\mathbf{\beta}) = \exp(\beta_0 + \beta_1x_1 + ... + \beta_px_p)
$$
The odds are the ratio of the probability of something happening versus not happening. If $\pi=0.5$, both events are equally likely and the odds are $1$. If the probability is higher, then the odds will be greater than $1$; if it is lower, the odds are below $1$. For example, if $\pi=0.75$, then our ratio is $0.75/0.25 = 3$. 

Often we are interested in how a change in the explanatory variables impacts the value of $\pi$. Usually we do this by keeping all variables fixed and incrementing one of them. For illustration, assume we have a simple case with two groups: those who received a drug (indicated with $i=1$), and those who did not (indicated with $i=0$). We are interested in $\pi_i$, the probability of survival of each group.

Our logistic regression model is:

$$
\log(\frac{\pi_i}{1-\pi_i}) = \beta_0 + \beta_1x_{i1}, \text{ for groups } i=\{0,1\}
$$

If someone did not receive a drug, then we have:

$$
\begin{aligned}
&\log(\frac{\pi_0}{1-\pi_0}) \\
&= \beta_0 + \beta_1(0) \\
&= \beta_0
\end{aligned}
$$

If they did, then we have:

$$
\log(\frac{\pi_1}{1-\pi_1}) = \beta_0 + \beta_1
$$

In this case we have:

* $\beta_0$, our log-odds of survival if someone did not receive a drug
* $\beta_1$, our log-odds of survival if someone did receive a drug

We can compare these two and use some algebra of logarithms to get our log odds ratio:

$$
\begin{aligned}
& \log(\frac{\pi_1}{1-\pi_1}) - \log(\frac{\pi_0}{1-\pi_0}) \\
&=\log(\frac{\pi_1/(1-\pi_1)}{\pi_0/(1-\pi_0)}) \\
&= (\beta_1 + \beta_0) - \beta_0 \\
&= \beta_1 \\
\end{aligned}
$$
In this case, $\beta_1$ is the log-odds ratio of survival for those given the drug versus not given the drug. To calculate the odds radio, we use $\exp(\beta_1)$. **We are usually most interested in this type of analysis**.

Finally, we can re-arrange everything to get our **probability of success**:

$$
\pi(\mathbf{x}) = \frac{\exp(\beta_0 + \beta_1x_1 + ... + \beta_px_p)}{1 + \exp(\beta_0 + \beta_1x_1 + ... + \beta_px_p)}
$$

Unlike odds, this value gives us the direct interpretation of how likely something is to happen. This can be useful to give more meaning to a rare event that may become more likely through a higher odds ratio.

For brevity, we will omit the derivations of various estimators. We use similar notation to linear regression, where $\hat{\beta}$ represents the estimate of $\beta$.

## The `glm` function

Logistic regression in R is done with the `glm` function. Its syntax and output are similar to the `lm` function, although we have to specify the family and link function. In this case, we will use the `binomial` family and `logit` link function. The typical setup is:

`model.name <- glm(RESPONSE ~ COVARIATES, family = binomial(link=logit), data = dataframename)`

As with linear regression, we can use the `summary` function, as well as `predict`.

### Example

Consider an example where we have 



Consider a simple case, where we have a contingency table of cardiovascular disease (CVD) in $3,112$ women from American Samoa. The women were classified as "obese" or "non-obese" and were tracked for whether they died of CVD. The data are available in the `Sleuth3` package (specifically `Sleuth3::case1801`). We want to test if obesity in this data is a significant predictor of CVD death.

Our table is:

```
#           Deaths NonDeaths
# Obese     16     2045
# NotObese  7      1044
```

```{r Obesity and CVD logistic model}
# Create a table out of our Deaths and NonDeaths 
cvd.data <- cbind(Sleuth3::case1801$Deaths,Sleuth3::case1801$NonDeaths)

# Model the contingency data
cvd.model <- glm(cvd.data ~ Sleuth3::case1801$Obesity, family = binomial(link = "logit"))

# Check the summary of our model
summary(cvd.model)

# Check the residual Deviance
print(paste("Residual Deviance", cvd.model$deviance))
```
Running our analysis, we see the Obesity level "Obese" has a p-value of $0.734$, which is not significant.

### Haberman data
We will use Haberman's survival data.^[Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.] It is a record of breast cancer patients who underwent surgery. The attributes are:

1. Age of patient at time of operation (numerical)
2. Patient's year of operation (year - 1900, numerical)
3. Number of positive axillary nodes detected (numerical)
4. Survival status (class attribute)

|   1 = the patient survived 5 years or longer
|   2 = the patient died within 5 years

```{r Read in Haberman data}
haberman.data <- read.csv("haberman.data", header=F)
colnames(haberman.data) <- c("age","surgery_year","nodes","survival_status")

# recode the survival status to 1 (lived 5+ years)  and 0 (died within 5 years)
haberman.data$survival_status <- ifelse(haberman.data$survival_status==2,0,1)
```

The first thing we may be interested in is plotting our data.

```{r Plot Haberman data against variables}
plot(haberman.data$nodes, haberman.data$survival_status)
plot(haberman.data$age, haberman.data$nodes)
plot(haberman.data$age, haberman.data$survival_status)
```


```{r}
haberman.model1 <- glm(survival_status ~ nodes,
                       family=binomial(link = "logit"),
                       data = haberman.data)

summary(haberman.model1)
```

```{r}
plot(haberman.data$nodes, haberman.data$survival_status, xlim = c(0,60))

newdata <- data.frame(nodes=seq(min(haberman.data$nodes), max(haberman.data$nodes),len=500))
newdata$pred = predict(haberman.model1, newdata, type="response")

lines(pred ~ nodes, newdata)
```


